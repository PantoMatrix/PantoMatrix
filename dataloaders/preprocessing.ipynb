{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.preprocessing import *\n",
    "from pymo.viz_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_list = {\n",
    "    \"beat_joints\" : {\n",
    "        'Hips':         [6,6],\n",
    "        'Spine':        [3,9],\n",
    "        'Spine1':       [3,12],\n",
    "        'Spine2':       [3,15],\n",
    "        'Spine3':       [3,18],\n",
    "        'Neck':         [3,21],\n",
    "        'Neck1':        [3,24],\n",
    "        'Head':         [3,27],\n",
    "        'HeadEnd':      [3,30],\n",
    "\n",
    "        'RShoulder':    [3,33], \n",
    "        'RArm':         [3,36],\n",
    "        'RArm1':        [3,39],\n",
    "        'RHand':        [3,42],    \n",
    "        'RHandM1':      [3,45],\n",
    "        'RHandM2':      [3,48],\n",
    "        'RHandM3':      [3,51],\n",
    "        'RHandM4':      [3,54],\n",
    "\n",
    "        'RHandR':       [3,57],\n",
    "        'RHandR1':      [3,60],\n",
    "        'RHandR2':      [3,63],\n",
    "        'RHandR3':      [3,66],\n",
    "        'RHandR4':      [3,69],\n",
    "\n",
    "        'RHandP':       [3,72],\n",
    "        'RHandP1':      [3,75],\n",
    "        'RHandP2':      [3,78],\n",
    "        'RHandP3':      [3,81],\n",
    "        'RHandP4':      [3,84],\n",
    "\n",
    "        'RHandI':       [3,87],\n",
    "        'RHandI1':      [3,90],\n",
    "        'RHandI2':      [3,93],\n",
    "        'RHandI3':      [3,96],\n",
    "        'RHandI4':      [3,99],\n",
    "\n",
    "        'RHandT1':      [3,102],\n",
    "        'RHandT2':      [3,105],\n",
    "        'RHandT3':      [3,108],\n",
    "        'RHandT4':      [3,111],\n",
    "\n",
    "        'LShoulder':    [3,114], \n",
    "        'LArm':         [3,117],\n",
    "        'LArm1':        [3,120],\n",
    "        'LHand':        [3,123],    \n",
    "        'LHandM1':      [3,126],\n",
    "        'LHandM2':      [3,129],\n",
    "        'LHandM3':      [3,132],\n",
    "        'LHandM4':      [3,135],\n",
    "\n",
    "        'LHandR':       [3,138],\n",
    "        'LHandR1':      [3,141],\n",
    "        'LHandR2':      [3,144],\n",
    "        'LHandR3':      [3,147],\n",
    "        'LHandR4':      [3,150],\n",
    "\n",
    "        'LHandP':       [3,153],\n",
    "        'LHandP1':      [3,156],\n",
    "        'LHandP2':      [3,159],\n",
    "        'LHandP3':      [3,162],\n",
    "        'LHandP4':      [3,165],\n",
    "\n",
    "        'LHandI':       [3,168],\n",
    "        'LHandI1':      [3,171],\n",
    "        'LHandI2':      [3,174],\n",
    "        'LHandI3':      [3,177],\n",
    "        'LHandI4':      [3,180],\n",
    "\n",
    "        'LHandT1':      [3,183],\n",
    "        'LHandT2':      [3,186],\n",
    "        'LHandT3':      [3,189],\n",
    "        'LHandT4':      [3,192],\n",
    "\n",
    "        'RUpLeg':       [3,195],\n",
    "        'RLeg':         [3,198],\n",
    "        'RFoot':        [3,201],\n",
    "        'RFootF':       [3,204],\n",
    "        'RToeBase':     [3,207],\n",
    "        'RToeBaseEnd':  [3,210],\n",
    "\n",
    "        'LUpLeg':       [3,213],\n",
    "        'LLeg':         [3,216],\n",
    "        'LFoot':        [3,219],\n",
    "        'LFootF':       [3,222],\n",
    "        'LToeBase':     [3,225],\n",
    "        'LToeBaseEnd':  [3,228],\n",
    "        },\n",
    "    \n",
    "    \"beat_141\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'RHand':       3 ,    \n",
    "            'RHandM1':     3 ,\n",
    "            'RHandM2':     3 ,\n",
    "            'RHandM3':     3 ,\n",
    "            'RHandR':      3 ,\n",
    "            'RHandR1':     3 ,\n",
    "            'RHandR2':     3 ,\n",
    "            'RHandR3':     3 ,\n",
    "            'RHandP':      3 ,\n",
    "            'RHandP1':     3 ,\n",
    "            'RHandP2':     3 ,\n",
    "            'RHandP3':     3 ,\n",
    "            'RHandI':      3 ,\n",
    "            'RHandI1':     3 ,\n",
    "            'RHandI2':     3 ,\n",
    "            'RHandI3':     3 ,\n",
    "            'RHandT1':     3 ,\n",
    "            'RHandT2':     3 ,\n",
    "            'RHandT3':     3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,\n",
    "            'LHand':       3 ,    \n",
    "            'LHandM1':     3 ,\n",
    "            'LHandM2':     3 ,\n",
    "            'LHandM3':     3 ,\n",
    "            'LHandR':      3 ,\n",
    "            'LHandR1':     3 ,\n",
    "            'LHandR2':     3 ,\n",
    "            'LHandR3':     3 ,\n",
    "            'LHandP':      3 ,\n",
    "            'LHandP1':     3 ,\n",
    "            'LHandP2':     3 ,\n",
    "            'LHandP3':     3 ,\n",
    "            'LHandI':      3 ,\n",
    "            'LHandI1':     3 ,\n",
    "            'LHandI2':     3 ,\n",
    "            'LHandI3':     3 ,\n",
    "            'LHandT1':     3 ,\n",
    "            'LHandT2':     3 ,\n",
    "            'LHandT3':     3 ,\n",
    "        },\n",
    "    \n",
    "    \"beat_27\" : {\n",
    "            'Spine':       3 ,\n",
    "            'Neck':        3 ,\n",
    "            'Neck1':       3 ,\n",
    "            'RShoulder':   3 , \n",
    "            'RArm':        3 ,\n",
    "            'RArm1':       3 ,\n",
    "            'LShoulder':   3 , \n",
    "            'LArm':        3 ,\n",
    "            'LArm1':       3 ,     \n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_fps: 15, reduce json 4, reduce bvh 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [1:42:03, 47.11s/it] \n",
      "130it [1:58:10, 54.54s/it] \n",
      "130it [1:41:38, 46.91s/it] \n",
      "130it [1:39:52, 46.10s/it] \n"
     ]
    }
   ],
   "source": [
    "#calculate mean and build cache for data. \n",
    "target_fps = 15\n",
    "ori_list = joint_list[\"beat_joints\"]\n",
    "target_list = joint_list[\"beat_141\"]\n",
    "ori_data_path = \"/home/ma-user/work/datasets/beat_rawdata_english/\"\n",
    "#wave cache from a = librosa.load(sr=16000) and np.save(a)\n",
    "ori_data_path_npy = \"/home/ma-user/work/datasets/beat_english_npy/\"\n",
    "ori_data_path_ann = \"/home/ma-user/work/datasets/beat_annotations_english/\"\n",
    "cache_path = f\"/home/ma-user/work/datasets/beat_cache/beat_4english_{target_fps}_141/\"\n",
    "reduce_factor_json = int(60/target_fps)\n",
    "reduce_factor_bvh = int(120/target_fps)\n",
    "print(f\"target_fps: {target_fps}, reduce json {reduce_factor_json}, reduce bvh {reduce_factor_bvh}\")\n",
    "speakers = sorted(os.listdir(ori_data_path),key=str,)\n",
    "\n",
    "npy_s_v = []\n",
    "npy_s_k = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "load_type = \"train\"\n",
    "if not os.path.exists(f\"{cache_path}\"): \n",
    "    os.mkdir(cache_path)\n",
    "if not os.path.exists(f\"{cache_path}{load_type}/\"): \n",
    "    os.mkdir(f\"{cache_path}{load_type}/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/wave16k/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_full/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/bvh_rot_vis/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/facial52/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/text/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/emo/\")\n",
    "    os.mkdir(f\"{cache_path}{load_type}/sem/\")     \n",
    "\n",
    "for speaker in [2,4,6,8]:#replace to 1, 31 for all speakers\n",
    "    all_data = os.listdir(ori_data_path_npy+str(speaker))\n",
    "    npy_all = []\n",
    "    json_all = []\n",
    "    bvh_all = []   \n",
    "    for ii, file in tqdm(enumerate(all_data)):\n",
    "        file = file[:-4]\n",
    "        shutil.copy(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\", f\"{cache_path}{load_type}/wave16k/{file}.npy\")\n",
    "        try:shutil.copy(f\"{ori_data_path}/{file.split('_')[0]}/{file}.TextGrid\", f\"{cache_path}{load_type}/text/{file}.TextGrid\")\n",
    "        except: print(f\"{file}.TextGrid\")\n",
    "        try: shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.txt\", f\"{cache_path}{load_type}/sem/{file}.txt\")\n",
    "        except: print(f\"{file}.txt\")\n",
    "        try: shutil.copy(f\"{ori_data_path_ann}/{file.split('_')[0]}/{file}.csv\", f\"{cache_path}{load_type}/emo/{file}.csv\")\n",
    "        except: print(f\"{file}.csv\")\n",
    "        npy_all.extend(list(np.load(f\"{ori_data_path_npy}/{file.split('_')[0]}/{file}.npy\")))\n",
    "\n",
    "        with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.json\", \"r\", encoding='utf-8') as json_file_raw:\n",
    "            json_file = json.load(json_file_raw)\n",
    "            with open(f\"{cache_path}{load_type}/facial52/{file}.json\", \"w\") as reduced_json:\n",
    "                counter = 0\n",
    "                new_frames_list = []\n",
    "                for json_data in json_file[\"frames\"]:\n",
    "                    json_all.append(json_data[\"weights\"])\n",
    "                    if counter % reduce_factor_json == 0:\n",
    "                        new_frames_list.append(json_data)\n",
    "                    counter += 1\n",
    "                json_new = {\"names\":json_file[\"names\"], \"frames\": new_frames_list}\n",
    "                json.dump(json_new, reduced_json)\n",
    "\n",
    "            with open(f\"{ori_data_path}/{file.split('_')[0]}/{file}.bvh\", \"r\") as bvh_file:\n",
    "                with open(f\"{cache_path}{load_type}/bvh_rot/{file}.bvh\", \"w\") as reduced_raw_bvh:\n",
    "                    with open(f\"{cache_path}{load_type}/bvh_full/{file}.bvh\", \"w\") as reduced_full_bvh:\n",
    "                        with open(f\"{cache_path}{load_type}/bvh_rot_vis/{file}.bvh\", \"w\") as reduced_trainable_bvh:\n",
    "                            for i, line_data in enumerate(bvh_file.readlines()):\n",
    "                                if i < 431: \n",
    "                                    reduced_full_bvh.write(line_data)\n",
    "                                    reduced_trainable_bvh.write(line_data)\n",
    "                                if i >= 431:\n",
    "                                    data = np.fromstring(line_data, dtype=float, sep=' ')\n",
    "                                    bvh_all.append(data)\n",
    "                                    if i % reduce_factor_bvh == 0:\n",
    "                                        reduced_full_bvh.write(line_data)\n",
    "                                        trainable_rotation = np.zeros_like(data)\n",
    "                                        for k, v in target_list.items():\n",
    "                                            trainable_rotation[ori_list[k][1]-v:ori_list[k][1]] = data[ori_list[k][1]-v:ori_list[k][1]]\n",
    "\n",
    "                                        trainable_line_data = np.array2string(trainable_rotation, max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_trainable_bvh.write(trainable_line_data[1:-2]+\"\\n\")\n",
    "                                        data_rotation = np.zeros((1))   \n",
    "                                        for k, v in target_list.items():\n",
    "                                            data_rotation = np.concatenate((data_rotation, data[ori_list[k][1]-v:ori_list[k][1]]))                             \n",
    "                                            raw_line_data = np.array2string(data_rotation[1:], max_line_width=np.inf, precision=6, suppress_small=False, separator=' ')\n",
    "                                        reduced_raw_bvh.write(raw_line_data[1:-2]+\"\\n\")\n",
    "\n",
    "    npy_all = np.array(npy_all)\n",
    "    npy_mean = np.mean(npy_all, axis=0)\n",
    "    npy_std = np.std(npy_all, axis=0)\n",
    "    npy_s_v.append([npy_mean, npy_std])\n",
    "    npy_s_k.append([len(npy_all)/16000/60])\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_mean_{speaker}.npy\", npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/wave16k/npy_std_{speaker}.npy\", npy_std)  \n",
    "\n",
    "    json_all = np.array(json_all)\n",
    "    json_mean = np.mean(json_all, axis=0)\n",
    "    json_std = np.std(json_all, axis=0)\n",
    "    json_s_v.append([json_mean, json_std])\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_mean_{speaker}.npy\", json_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/facial52/json_std_{speaker}.npy\", json_std)\n",
    "\n",
    "    bvh_all = np.array(bvh_all)\n",
    "    bvh_mean = np.mean(bvh_all, axis=0)\n",
    "    bvh_std = np.std(bvh_all, axis=0)\n",
    "    bvh_s_v.append([bvh_mean, bvh_std])\n",
    "    # number of joints reduced version \n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_mean[ori_list[k][1]-v:ori_list[k][1]]))\n",
    "    new_npy_mean = data_rotation[1:]\n",
    "    data_rotation = np.zeros((1))\n",
    "    for k, v in target_list.items():\n",
    "        data_rotation = np.concatenate((data_rotation, bvh_std[ori_list[k][1]-v:ori_list[k][1]]))\n",
    "    new_npy_std = data_rotation[1:]\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_mean_{speaker}.npy\", new_npy_mean)\n",
    "    np.save(f\"{cache_path}{load_type}/bvh_rot/bvh_std_{speaker}.npy\", new_npy_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " audio mean: -1.7519207176519558e-05, std: 0.08796644119702024\n",
      " json mean: [0.07903196 0.07907486 0.1966437  0.07479544 0.07471471 0.02340628\n",
      " 0.18603497 0.19610078 0.12443771 0.12374972 0.09295963 0.0928695\n",
      " 0.06626222 0.14945787 0.07691047 0.03130532 0.08498158 0.08467752\n",
      " 0.24461904 0.244762   0.14221006 0.14197496 0.14244319 0.01448534\n",
      " 0.175984   0.01600465 0.10404965 0.13908137 0.13762701 0.0028755\n",
      " 0.00586368 0.23009334 0.01440475 0.29456463 0.3070315  0.06273843\n",
      " 0.0675392  0.07001068 0.00665162 0.05176206 0.05518028 0.06045766\n",
      " 0.26084122 0.27372042 0.23230666 0.27026909 0.28944367 0.17446793\n",
      " 0.16001544 0.21024718 0.20605796], std: [0.10819756 0.10822419 0.10085156 0.11509224 0.11499131 0.01325863\n",
      " 0.08551676 0.08363134 0.21731856 0.21597039 0.12406761 0.12393432\n",
      " 0.12257504 0.1480999  0.12704492 0.09446166 0.12722647 0.12663313\n",
      " 0.10571671 0.10562905 0.15859903 0.15843439 0.07354202 0.02665617\n",
      " 0.09462479 0.02562761 0.06072847 0.05251625 0.05131506 0.01838467\n",
      " 0.0247272  0.12950437 0.01633449 0.11599335 0.11963    0.03285215\n",
      " 0.03465722 0.04733075 0.01288929 0.04753016 0.04595906 0.04098985\n",
      " 0.07751192 0.20044277 0.19645004 0.0668491  0.07121049 0.10874268\n",
      " 0.09668467 0.09903212 0.094696  ]\n",
      "(141,)\n",
      "(141,)\n"
     ]
    }
   ],
   "source": [
    "# calculate global mean and std for average all speakers\n",
    "npy_s_v = []\n",
    "json_s_v = []\n",
    "bvh_s_v = []\n",
    "\n",
    "npy_path = f\"{cache_path}{load_type}/wave16k/\"\n",
    "bvh_path = f\"{cache_path}{load_type}/bvh_rot/\"\n",
    "json_path = f\"{cache_path}{load_type}/facial52/\" \n",
    "        \n",
    "for i in [2,4,6,8]:\n",
    "    npy_s_v.append([np.load(f\"{npy_path}npy_mean_{i}.npy\"), np.load(f\"{npy_path}npy_std_{i}.npy\")])\n",
    "    json_s_v.append([np.load(f\"{json_path}json_mean_{i}.npy\"), np.load(f\"{json_path}json_std_{i}.npy\")])\n",
    "    bvh_s_v.append([np.load(f\"{bvh_path}bvh_mean_{i}.npy\"), np.load(f\"{bvh_path}bvh_std_{i}.npy\")])\n",
    "\n",
    "all_length = 0\n",
    "new_m = np.zeros_like(npy_s_v[0][0])\n",
    "new_s = np.zeros_like(npy_s_v[0][0])\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(npy_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "new_s /= all_length\n",
    "new_s = math.sqrt(new_s)\n",
    "print(f\" audio mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{npy_path}npy_mean.npy\", new_m)\n",
    "np.save(f\"{npy_path}/npy_std.npy\", new_s)  \n",
    "\n",
    "new_m = np.zeros_like(json_s_v[0][0])\n",
    "new_s = np.zeros_like(json_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(json_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "new_s /= all_length\n",
    "new_s = math.sqrt(new_s)\n",
    "print(f\" json mean: {new_m}, std: {new_s}\") \n",
    "np.save(f\"{json_path}json_mean.npy\", new_m)\n",
    "np.save(f\"{json_path}/json_std.npy\", new_s)\n",
    "\n",
    "new_m = np.zeros_like(bvh_s_v[0][0])\n",
    "new_s = np.zeros_like(bvh_s_v[0][0])\n",
    "all_length = 0\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    all_length += npy_s_k[i][0]\n",
    "    new_m += npy_s_k[i][0] * m\n",
    "new_m /= all_length\n",
    "for i, (m, s) in enumerate(bvh_s_v):\n",
    "    new_s += ((s**2) + (m-new_m)**2) * npy_s_k[i][0]\n",
    "new_s /= all_length\n",
    "new_s = math.sqrt(new_s)\n",
    "\n",
    "print(new_m.shape)\n",
    "print(new_s.shape)\n",
    "np.save(f\"{bvh_path}bvh_mean.npy\", new_m)\n",
    "np.save(f\"{bvh_path}/bvh_std.npy\", new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rule_english = {\n",
    "    # 4h speakers x 10\n",
    "    \"1, 2, 3, 4, 6, 7, 8, 9, 11, 21\":{\n",
    "        # 48+40+100=188mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_17_17\", \"0_18_18\", \"0_19_19\", \"0_20_20\", \"0_21_21\", \"0_22_22\", \"0_23_23\", \"0_24_24\", \\\n",
    "            \"0_25_25\", \"0_26_26\", \"0_27_27\", \"0_28_28\", \"0_29_29\", \"0_30_30\", \"0_31_31\", \"0_32_32\", \\\n",
    "            \"0_33_33\", \"0_34_34\", \"0_35_35\", \"0_36_36\", \"0_37_37\", \"0_38_38\", \"0_39_39\", \"0_40_40\", \\\n",
    "            \"0_41_41\", \"0_42_42\", \"0_43_43\", \"0_44_44\", \"0_45_45\", \"0_46_46\", \"0_47_47\", \"0_48_48\", \\\n",
    "            \"0_49_49\", \"0_50_50\", \"0_51_51\", \"0_52_52\", \"0_53_53\", \"0_54_54\", \"0_55_55\", \"0_56_56\", \\\n",
    "            \n",
    "            \"0_66_66\", \"0_67_67\", \"0_68_68\", \"0_69_69\", \"0_70_70\", \"0_71_71\",  \\\n",
    "            \"0_74_74\", \"0_75_75\", \"0_76_76\", \"0_77_77\", \"0_78_78\", \"0_79_79\",  \\\n",
    "            \"0_82_82\", \"0_83_83\", \"0_84_84\", \"0_85_85\",  \\\n",
    "            \"0_88_88\", \"0_89_89\", \"0_90_90\", \"0_91_91\", \"0_92_92\", \"0_93_93\",  \\\n",
    "            \"0_96_96\", \"0_97_97\", \"0_98_98\", \"0_99_99\", \"0_100_100\", \"0_101_101\",  \\\n",
    "            \"0_104_104\", \"0_105_105\", \"0_106_106\", \"0_107_107\", \"0_108_108\", \"0_109_109\",  \\\n",
    "            \"0_112_112\", \"0_113_113\", \"0_114_114\", \"0_115_115\", \"0_116_116\", \"0_117_117\",  \\\n",
    "            \n",
    "            \"1_2_2\", \"1_3_3\", \"1_4_4\", \"1_5_5\", \"1_6_6\", \"1_7_7\", \"1_8_8\", \"1_9_9\", \"1_10_10\", \"1_11_11\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"val\": [\n",
    "            \"0_57_57\", \"0_58_58\", \"0_59_59\", \"0_60_60\", \"0_61_61\", \"0_62_62\", \"0_63_63\", \"0_64_64\", \\\n",
    "            \"0_72_72\", \"0_80_80\", \"0_86_86\", \"0_94_94\", \"0_102_102\", \"0_110_110\", \"0_118_118\", \\\n",
    "            \"1_12_12\",\n",
    "        ],\n",
    "        # 8+7+10=25mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\", \\\n",
    "           \"0_65_65\", \"0_73_73\", \"0_81_81\", \"0_87_87\", \"0_95_95\", \"0_103_103\", \"0_111_111\", \\\n",
    "           \"1_1_1\",\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    # 1h speakers x 20\n",
    "    \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\":{\n",
    "        # 8+7+20=35mins each\n",
    "        \"train\": [\n",
    "            \"0_9_9\", \"0_10_10\", \"0_11_11\", \"0_12_12\", \"0_13_13\", \"0_14_14\", \"0_15_15\", \"0_16_16\", \\\n",
    "            \"0_66_66\", \"0_74_74\", \"0_82_82\", \"0_88_88\", \"0_96_96\", \"0_104_104\", \"0_112_112\", \"0_118_118\", \\\n",
    "            \"1_2_2\", \"1_3_3\", \n",
    "            \"1_0_0\", \"1_4_4\", # for speaker 29 only\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        # 0_65_a and 0_65_b denote the frist and second half of sequence 0_65_65\n",
    "        \"val\": [\n",
    "            \"0_5_5\", \"0_6_6\", \"0_7_7\", \"0_8_8\",  \\\n",
    "            \"0_65_b\", \"0_73_b\", \"0_81_b\", \"0_87_b\", \"0_95_b\", \"0_103_b\", \"0_111_b\", \\\n",
    "            \"1_1_b\",\n",
    "        ],\n",
    "        # 4+3.5+5 = 12.5mins each\n",
    "        \"test\": [\n",
    "           \"0_1_1\", \"0_2_2\", \"0_3_3\", \"0_4_4\", \\\n",
    "           \"0_65_a\", \"0_73_a\", \"0_81_a\", \"0_87_a\", \"0_95_a\", \"0_103_a\", \"0_111_a\", \\\n",
    "           \"1_1_a\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequence(source_path, save_path_a, save_path_b, file_id, fps = 15, sr = 16000, tmp=\"/home/ma-user/work/datasets/beat_tmp/\"):\n",
    "    if not os.path.exists(tmp): os.mkdir(tmp)\n",
    "    cut_point = 30 if file_id.split(\"_\")[0] == \"0\" else 300 #in seconds\n",
    "    if source_path.endswith(\".npy\"):\n",
    "        data = np.load(source_path)\n",
    "        data_a = data[:sr*cut_point]\n",
    "        data_b = data[sr*cut_point:]\n",
    "        np.save(save_path_a, data_a)\n",
    "        np.save(save_path_b, data_b)\n",
    "        \n",
    "    elif source_path.endswith(\".bvh\"):\n",
    "        copy_lines = 431 if \"full\" in source_path or \"vis\" in source_path else 0\n",
    "        with open(source_path, \"r\") as data:\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    for i, line_data in enumerate(data.readlines()):\n",
    "                        if i < copy_lines:\n",
    "                            data_a.write(line_data)\n",
    "                            data_b.write(line_data)\n",
    "                        elif i < cut_point * fps:\n",
    "                            data_a.write(line_data)\n",
    "                        else:\n",
    "                            data_b.write(line_data)\n",
    "    \n",
    "    elif source_path.endswith(\".json\"):\n",
    "        with open(source_path, \"r\", encoding='utf-8') as data:\n",
    "            json_file = json.load(data)\n",
    "            with open(save_path_a, \"w\") as data_a:\n",
    "                with open(save_path_b, \"w\") as data_b:\n",
    "                    new_frames_a = []\n",
    "                    new_frames_b = []\n",
    "                    for json_data in json_file[\"frames\"]:\n",
    "                        if json_data[\"time\"] < cut_point:\n",
    "                            new_frames_a.append(json_data)\n",
    "                        else:\n",
    "                            new_frame = json_data.copy()\n",
    "                            new_frame[\"time\"]-=cut_point\n",
    "                            new_frames_b.append(new_frame)\n",
    "                    json_new_a = {\"names\":json_file[\"names\"], \"frames\": new_frames_a}\n",
    "                    json_new_b = {\"names\":json_file[\"names\"], \"frames\": new_frames_b}\n",
    "                    json.dump(json_new_a, data_a)\n",
    "                    json.dump(json_new_b, data_b) \n",
    "        \n",
    "    else:\n",
    "        # processing in the dataloader\n",
    "        shutil.copy(source_path, save_path_a)\n",
    "        shutil.copy(source_path, save_path_b)\n",
    "    try:\n",
    "        shutil.move(source_path, tmp)\n",
    "    except:\n",
    "        print(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 99.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# spilt data\n",
    "speaker_names = [\n",
    "    \"wayne\", \"scott\", \"solomon\", \"lawrence\", \"stewart\", \"carla\", \"sophie\", \"catherine\", \"miranda\", \"kieks\", \\\n",
    "    \"nidal\", \"zhao\", \"lu\", \"zhang\", \"carlos\", \"jorge\", \"itoi\", \"daiki\", \"jaime\", \"li\", \\\n",
    "    \"ayana\", \"luqi\", \"hailing\", \"kexin\", \"goto\", \"reamey\", \"yingqing\", \"tiffnay\", \"hanieh\", \"katya\",\n",
    "]\n",
    "default_path = \"/home/ma-user/work/datasets/beat_cache/beat_4english_15_141/train\"\n",
    "four_hour_speakers = \"1, 2, 3, 4, 6, 7, 8, 9, 11, 21\".split(\", \")\n",
    "one_hour_speakers = \"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\".split(\", \")\n",
    "folders = os.listdir(default_path)\n",
    "if not os.path.exists(default_path.replace(\"train\", \"val\")): os.mkdir(default_path.replace(\"train\", \"val\"))\n",
    "if not os.path.exists(default_path.replace(\"train\", \"test\")): os.mkdir(default_path.replace(\"train\", \"test\"))\n",
    "endwith = []\n",
    "for folder in folders:\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"val\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"val\")+\"/\"+folder)\n",
    "    if not os.path.exists(default_path.replace(\"train\", \"test\")+\"/\"+folder): os.mkdir(default_path.replace(\"train\", \"test\")+\"/\"+folder)\n",
    "    endwith.append(os.listdir(default_path+\"/\"+folder)[500].split(\".\")[-1])\n",
    "    \n",
    "for speaker_id in tqdm([2,4,6,8]):\n",
    "    val = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"val\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11, 21\"][\"val\"]\n",
    "    test = split_rule_english[\"5, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30\"][\"test\"] if str(speaker_id) in one_hour_speakers else split_rule_english[\"1, 2, 3, 4, 6, 7, 8, 9, 11, 21\"][\"test\"]\n",
    "    for file_id in val:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"b\" in file_id:\n",
    "                cut_sequence(\n",
    "                    source_path=f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_{file_id.split('_')[1]}.{endwith[ide]}\",\n",
    "                    save_path_a=f\"{default_path.replace('train', 'test')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_a.{endwith[ide]}\",\n",
    "                    save_path_b=f\"{default_path.replace('train', 'val')}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id.split('_')[0]}_{file_id.split('_')[1]}_b.{endwith[ide]}\",\n",
    "                    file_id = file_id,\n",
    "                        )\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'val')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n",
    "    for file_id in test:\n",
    "        for ide, folder in enumerate(folders):\n",
    "            if \"a\" in file_id:\n",
    "                pass\n",
    "            else:\n",
    "                #pass\n",
    "                try:\n",
    "                    shutil.move(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\", f\"{default_path.replace('train', 'test')}/{folder}/\")\n",
    "                except:\n",
    "                    print(f\"{default_path}/{folder}/{speaker_id}_{speaker_names[speaker_id-1]}_{file_id}.{endwith[ide]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rot2pos\n",
    "# import time\n",
    "# start_t = time.time()\n",
    "# p = BVHParser()\n",
    "# data = [p.parse(\"../../../datasets/beat_full/1/1_wayne_0_1_8.bvh\")]\n",
    "# dr_pipe = Pipeline([\n",
    "#     ('param', MocapParameterizer('position')),\n",
    "# ])\n",
    "# xx = dr_pipe.fit_transform(data)\n",
    "# # data[0].values.shape\n",
    "# df = xx[0].values.head(-1)\n",
    "# print((time.time()-start_t)/60)\n",
    "# data_list = []\n",
    "# p_in_f = []\n",
    "# for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#     x = df['%s_Xposition'%joint][-1]\n",
    "#     y = df['%s_Yposition'%joint][-1]\n",
    "#     z = df['%s_Zposition'%joint][-1]\n",
    "#     p = [x, y, z]\n",
    "#     p_in_f.append(p)\n",
    "# data_list.append(p_in_f)\n",
    "\n",
    "# %matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import matplotlib.pyplot as plt\n",
    "# draw = np.array(data_list[0])\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(draw.T[0], draw.T[1], draw.T[2], s=100, c='r')\n",
    "# plt.show()\n",
    "\n",
    "# json_str = {}\n",
    "# path = '../../../datasets/beat_full/'\n",
    "# for i in range(30):\n",
    "#     for bvh in os.listdir(path+str(i+1)+\"/\"):\n",
    "#         if bvh.endswith('.bvh'):\n",
    "#             p = BVHParser()\n",
    "#             filename = os.path.join(path, str(i+1), bvh)\n",
    "#             print(filename)\n",
    "#             data = [p.parse(filename)]\n",
    "\n",
    "#             dr_pipe = Pipeline([\n",
    "#                 ('param', MocapParameterizer('position')),\n",
    "#             ])\n",
    "\n",
    "#             xx = dr_pipe.fit_transform(data)\n",
    "#             data[0].values.shape\n",
    "#             df = xx[0].values.head(-1)\n",
    "\n",
    "#             data_list = []\n",
    "#             #counter = 0\n",
    "#             for f in range(data[0].values.shape[0]):\n",
    "#                 #if counter == 100: break\n",
    "#                 p_in_f = []\n",
    "#                 for joint, values in joint_list[\"beat_joints\"].items():\n",
    "#                     # print(joint)\n",
    "#                     x = df['%s_Xposition'%joint][f-1]\n",
    "#                     y = df['%s_Yposition'%joint][f-1]\n",
    "#                     z = df['%s_Zposition'%joint][f-1]\n",
    "#                     p = [x, y, z]\n",
    "#                     p_in_f.append(p)\n",
    "#                 #counter += 1\n",
    "#                 data_list.append(p_in_f)\n",
    "#             # print(np.array(data_list).shape)\n",
    "#             json_str[filename] = data_list\n",
    "#             print(json_str)\n",
    "#             # json_string = json.dumps(json_str)\n",
    "#             with open('position_data2.json', 'w') as outfile:\n",
    "#                 json.dump(json_str, outfile)\n",
    "#             break\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29dda2bfdd4e457e82e82db4aec4a5a63c7b9f76350636fa7548ef8e54df0818"
  },
  "kernelspec": {
   "display_name": "TensorFlow-2.1.0",
   "language": "python",
   "name": "tensorflow-2.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
